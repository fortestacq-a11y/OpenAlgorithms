export interface Algorithm {
  slug: string;
  name: string;
  category: string;
  description: string;
  complexity: { time: string; space: string };
  useCases?: string[];
  pros?: string[];
  cons?: string[];
}

export const algorithms: Algorithm[] = [
  // ─── Sorting ─────────────────────────────────────────────────────────────
  {
    slug: "bubble-sort",
    name: "Bubble Sort",
    category: "sorting",
    description: "A simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order.",
    complexity: { time: "O(n²)", space: "O(1)" },
  },
  {
    slug: "selection-sort",
    name: "Selection Sort",
    category: "sorting",
    description: "Sorts an array by repeatedly finding the minimum element from unsorted part and putting it at the beginning.",
    complexity: { time: "O(n²)", space: "O(1)" },
  },
  {
    slug: "insertion-sort",
    name: "Insertion Sort",
    category: "sorting",
    description: "Builds the final sorted array one item at a time. It is much less efficient on large lists than more advanced algorithms.",
    complexity: { time: "O(n²)", space: "O(1)" },
  },
  {
    slug: "quick-sort",
    name: "Quick Sort",
    category: "sorting",
    description: "A divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays.",
    complexity: { time: "O(n log n)", space: "O(log n)" },
  },
  {
    slug: "merge-sort",
    name: "Merge Sort",
    category: "sorting",
    description: "A divide-and-conquer algorithm that divides the input array into two halves, calls itself for the two halves, and then merges the two sorted halves.",
    complexity: { time: "O(n log n)", space: "O(n)" },
  },

  // ─── Searching ───────────────────────────────────────────────────────────
  {
    slug: "linear-search",
    name: "Linear Search",
    category: "searching",
    description: "Sequentially checks each element of the list until a match is found or the whole list has been searched.",
    complexity: { time: "O(n)", space: "O(1)" },
  },
  {
    slug: "binary-search",
    name: "Binary Search",
    category: "searching",
    description: "Search a sorted array by repeatedly dividing the search interval in half.",
    complexity: { time: "O(log n)", space: "O(1)" },
  },

  // ─── Graph ───────────────────────────────────────────────────────────────
  {
    slug: "bfs",
    name: "Breadth First Search",
    category: "graph",
    description: "Traverses a graph level by level, visiting all neighbors of a node before moving to the next level.",
    complexity: { time: "O(V + E)", space: "O(V)" },
  },
  {
    slug: "dfs",
    name: "Depth First Search",
    category: "graph",
    description: "Traverses a graph by exploring as far as possible along each branch before backtracking.",
    complexity: { time: "O(V + E)", space: "O(V)" },
  },

  // ─── Supervised: Regression ──────────────────────────────────────────────
  {
    slug: "linear-regression",
    name: "Linear Regression",
    category: "regression",
    description: "Models the relationship between a dependent variable and one or more independent variables using a linear equation. It finds the best-fit line that minimizes the sum of squared residuals.",
    complexity: { time: "O(n·d²)", space: "O(d²)" },
    useCases: ["House price prediction", "Stock price forecasting", "Sales estimation", "Risk assessment"],
    pros: ["Simple and interpretable", "Fast to train", "No hyperparameters", "Works well with linear relationships"],
    cons: ["Assumes linearity", "Sensitive to outliers", "Struggles with feature correlation", "Cannot model complex patterns"],
  },
  {
    slug: "polynomial-regression",
    name: "Polynomial Regression",
    category: "regression",
    description: "An extension of linear regression where the relationship between the independent variable and dependent variable is modeled as an nth-degree polynomial.",
    complexity: { time: "O(n·d²)", space: "O(d²)" },
    useCases: ["Growth rate modeling", "Epidemiology curves", "Physics experiments", "Non-linear trend fitting"],
    pros: ["Captures non-linear relationships", "Flexible degree of fit", "No need for transformations", "Good for curved data"],
    cons: ["Prone to overfitting with high degree", "Sensitive to outliers", "Extrapolation is unreliable", "Feature scaling required"],
  },
  {
    slug: "ridge-regression",
    name: "Ridge Regression",
    category: "regression",
    description: "A regularized version of linear regression that adds an L2 penalty term to the loss function, shrinking coefficients to prevent overfitting.",
    complexity: { time: "O(n·d²)", space: "O(d²)" },
    useCases: ["High-dimensional datasets", "Multicollinear features", "Genomics", "Financial modeling"],
    pros: ["Handles multicollinearity well", "Reduces overfitting", "Stable coefficients", "Works with correlated features"],
    cons: ["Does not perform feature selection", "Requires tuning alpha", "Less interpretable than OLS", "Assumes linear relationship"],
  },
  {
    slug: "lasso-regression",
    name: "Lasso Regression",
    category: "regression",
    description: "Regularized linear regression using an L1 penalty that can shrink some coefficients to exactly zero, performing automatic feature selection.",
    complexity: { time: "O(n·d·k)", space: "O(d)" },
    useCases: ["Feature selection", "Sparse models", "Gene expression analysis", "Text regression"],
    pros: ["Automatic feature selection", "Produces sparse models", "Handles irrelevant features", "More interpretable"],
    cons: ["Selects only one feature from correlated group", "Biased estimates", "Sensitive to scaling", "May underfit with few features"],
  },
  {
    slug: "elastic-net",
    name: "Elastic Net",
    category: "regression",
    description: "Combines L1 (Lasso) and L2 (Ridge) regularization penalties. Useful when there are multiple correlated features, balancing between feature selection and coefficient shrinkage.",
    complexity: { time: "O(n·d·k)", space: "O(d)" },
    useCases: ["High-dimensional data with correlated features", "Genomics", "NLP regression", "Finance"],
    pros: ["Handles correlated features", "Feature selection + shrinkage", "Flexible via two hyperparameters", "Robust to multicollinearity"],
    cons: ["Two hyperparameters to tune", "Slower than Lasso/Ridge", "Less interpretable", "Assumes linearity"],
  },
  {
    slug: "svr",
    name: "Support Vector Regression (SVR)",
    category: "regression",
    description: "Applies Support Vector Machine principles to regression. It finds a function within an epsilon-margin of the true values, using kernel tricks to handle non-linear data.",
    complexity: { time: "O(n² to n³)", space: "O(n)" },
    useCases: ["Small to medium datasets", "Non-linear regression", "Energy forecasting", "Financial prediction"],
    pros: ["Robust to outliers", "Handles non-linear data via kernels", "Effective in high dimensions", "Generalizes well"],
    cons: ["Slow on large datasets", "Sensitive to feature scaling", "Kernel and hyperparameter selection difficult", "Black-box model"],
  },
  {
    slug: "decision-tree-regression",
    name: "Decision Tree Regression",
    category: "regression",
    description: "A non-parametric supervised learning method that predicts by learning decision rules from features. It partitions the feature space into rectangles and fits a constant in each region.",
    complexity: { time: "O(n·d·log n)", space: "O(n)" },
    useCases: ["Non-linear relationships", "Feature interaction detection", "Medical diagnosis", "Customer segmentation"],
    pros: ["Interpretable and visualizable", "No feature scaling needed", "Handles non-linear data", "Feature importance"],
    cons: ["Prone to overfitting", "Unstable (high variance)", "Biased with unbalanced data", "Not smooth predictions"],
  },
  {
    slug: "random-forest-regression",
    name: "Random Forest Regression",
    category: "regression",
    description: "An ensemble of decision trees trained on random subsets of data and features. Predictions are averaged across all trees, reducing variance and improving generalization.",
    complexity: { time: "O(n·d·log n·T)", space: "O(n·T)" },
    useCases: ["Real estate pricing", "Stock market prediction", "Medical data", "Ecological modeling"],
    pros: ["High accuracy", "Robust to overfitting", "Handles missing values", "Feature importance ranking"],
    cons: ["Less interpretable", "Slow to predict on large models", "Many hyperparameters", "Memory intensive"],
  },
  {
    slug: "gradient-boosting-regression",
    name: "Gradient Boosting Regression",
    category: "regression",
    description: "An ensemble method that builds trees sequentially, each correcting the errors of the previous one by fitting residuals. Uses gradient descent in function space.",
    complexity: { time: "O(n·d·T·log n)", space: "O(n·T)" },
    useCases: ["Competition winning tasks", "Financial forecasting", "Web ranking", "Click-through rate prediction"],
    pros: ["High predictive accuracy", "Handles mixed data types", "Robust feature importance", "Flexible loss functions"],
    cons: ["Prone to overfitting without tuning", "Slow training", "Many hyperparameters", "Less interpretable"],
  },
  {
    slug: "xgboost-regression",
    name: "XGBoost",
    category: "regression",
    description: "An optimized distributed gradient boosting library. Uses regularization, parallel processing, and hardware optimization to deliver fast and accurate gradient boosting.",
    complexity: { time: "O(n·d·T·log n)", space: "O(n·T)" },
    useCases: ["Kaggle competitions", "Fraud detection", "Drug discovery", "Insurance risk scoring"],
    pros: ["State-of-the-art accuracy", "Built-in regularization", "Handles missing data", "Very fast"],
    cons: ["Requires careful hyperparameter tuning", "Memory intensive", "Can overfit", "Less interpretable than trees"],
  },

  // ─── Supervised: Classification ──────────────────────────────────────────
  {
    slug: "logistic-regression",
    name: "Logistic Regression",
    category: "classification",
    description: "A statistical model that models the probability of a binary outcome using the logistic (sigmoid) function. Despite its name, it is a classification algorithm.",
    complexity: { time: "O(n·d·k)", space: "O(d)" },
    useCases: ["Email spam detection", "Disease diagnosis", "Credit scoring", "Customer churn"],
    pros: ["Probabilistic output", "Fast and interpretable", "No hyperparameters by default", "Works well on linearly separable data"],
    cons: ["Assumes linearity in log-odds", "Sensitive to outliers", "Cannot capture complex patterns", "Needs large sample size"],
  },
  {
    slug: "knn",
    name: "K-Nearest Neighbors (KNN)",
    category: "classification",
    description: "A non-parametric algorithm that classifies a point based on the majority class of its K nearest neighbors in the feature space.",
    complexity: { time: "O(n·d) per query", space: "O(n·d)" },
    useCases: ["Recommendation systems", "Image recognition", "Anomaly detection", "Medical diagnosis"],
    pros: ["No training phase", "Simple to understand", "Naturally handles multi-class", "Adapts to new training data"],
    cons: ["Slow at prediction time", "Sensitive to irrelevant features", "Requires feature scaling", "High memory usage"],
  },
  {
    slug: "svm",
    name: "Support Vector Machine (SVM)",
    category: "classification",
    description: "Finds the optimal hyperplane that maximizes the margin between classes. Uses kernel functions to handle non-linearly separable data.",
    complexity: { time: "O(n² to n³)", space: "O(n)" },
    useCases: ["Image classification", "Text classification", "Bioinformatics", "Face detection"],
    pros: ["Effective in high dimensions", "Robust to overfitting", "Versatile with kernels", "Works with small datasets"],
    cons: ["Slow on large datasets", "Sensitive to feature scaling", "Difficult to interpret", "Kernel selection is tricky"],
  },
  {
    slug: "decision-tree",
    name: "Decision Tree",
    category: "classification",
    description: "A tree-structured model that makes decisions by splitting data on feature values. Each internal node is a feature test, each leaf is a class label.",
    complexity: { time: "O(n·d·log n)", space: "O(n)" },
    useCases: ["Medical diagnosis", "Customer segmentation", "Fraud detection", "Rule extraction"],
    pros: ["Highly interpretable", "No scaling needed", "Handles categorical data", "Fast prediction"],
    cons: ["High variance (unstable)", "Prone to overfitting", "Greedy splits not optimal", "Biased with unbalanced classes"],
  },
  {
    slug: "random-forest",
    name: "Random Forest",
    category: "classification",
    description: "An ensemble of decision trees using bootstrap aggregating (bagging) and random feature selection. Majority vote determines the final class.",
    complexity: { time: "O(n·d·log n·T)", space: "O(n·T)" },
    useCases: ["Remote sensing", "Financial fraud", "Medical imaging", "Customer churn"],
    pros: ["Highly accurate", "Handles overfitting", "Robust to noise", "Built-in feature importance"],
    cons: ["Low interpretability", "Slow for real-time prediction", "Memory intensive", "Requires tuning"],
  },
  {
    slug: "naive-bayes",
    name: "Naive Bayes",
    category: "classification",
    description: "A probabilistic classifier based on Bayes' theorem with the 'naive' assumption of feature independence. Extremely fast and effective for text classification.",
    complexity: { time: "O(n·d)", space: "O(d·c)" },
    useCases: ["Spam filtering", "Sentiment analysis", "Document classification", "Real-time prediction"],
    pros: ["Very fast training", "Works well with small data", "Handles high dimensions", "Probabilistic output"],
    cons: ["Independence assumption rarely holds", "Cannot learn feature interactions", "Biased class probabilities", "Poor with correlated features"],
  },
  {
    slug: "gradient-boosting",
    name: "Gradient Boosting",
    category: "classification",
    description: "Sequentially builds a strong classifier from weak learners (shallow trees) by minimizing a loss function using gradient descent in function space.",
    complexity: { time: "O(n·d·T·log n)", space: "O(n·T)" },
    useCases: ["Web search ranking", "Click prediction", "Fraud detection", "Medical risk scoring"],
    pros: ["Excellent accuracy", "Flexible loss functions", "Handles missing values", "Feature importance"],
    cons: ["Prone to overfitting", "Computationally expensive", "Many hyperparameters", "Sequential, hard to parallelize"],
  },
  {
    slug: "adaboost",
    name: "AdaBoost",
    category: "classification",
    description: "Adaptive Boosting trains a sequence of weak classifiers. Each iteration reweights misclassified samples so the next classifier focuses on harder examples.",
    complexity: { time: "O(n·d·T)", space: "O(n)" },
    useCases: ["Face detection", "Binary classification", "Medical diagnosis", "Text categorization"],
    pros: ["Less prone to overfitting than single tree", "Easy to implement", "Few hyperparameters", "Interpretable weak learners"],
    cons: ["Sensitive to noisy data and outliers", "Slow with many weak learners", "Binary by default", "Requires base learner tuning"],
  },
  {
    slug: "xgboost",
    name: "XGBoost",
    category: "classification",
    description: "An optimized gradient boosting framework with regularization, parallel computation, and cache-aware algorithms. Consistently wins machine learning competitions.",
    complexity: { time: "O(n·d·T·log n)", space: "O(n·T)" },
    useCases: ["Kaggle competitions", "Click-through prediction", "Customer churn", "Intrusion detection"],
    pros: ["State-of-the-art accuracy", "Fast and scalable", "Handles missing data", "Built-in regularization"],
    cons: ["Many hyperparameters", "Less interpretable", "High memory usage", "Overfitting if not regularized"],
  },
  {
    slug: "lightgbm",
    name: "LightGBM",
    category: "classification",
    description: "A gradient boosting framework using leaf-wise tree growth and histogram-based algorithms. Designed for efficiency on large-scale datasets.",
    complexity: { time: "O(n·T·log n)", space: "O(n·T)" },
    useCases: ["Large-scale ranking", "High-dimensional classification", "Click prediction", "Financial scoring"],
    pros: ["Extremely fast training", "Low memory usage", "High accuracy", "Supports categorical features natively"],
    cons: ["Can overfit on small data", "Leaf-wise growth needs tuning", "Less tested than XGBoost", "Sensitive to hyperparameters"],
  },
  {
    slug: "catboost",
    name: "CatBoost",
    category: "classification",
    description: "A gradient boosting library optimized for categorical features. Uses ordered boosting to reduce overfitting on small datasets.",
    complexity: { time: "O(n·T·log n)", space: "O(n·T)" },
    useCases: ["Recommendation systems", "Financial modeling", "Retail analytics", "High categorical feature data"],
    pros: ["Native categorical feature support", "Minimal preprocessing", "Robust to overfitting", "GPU support"],
    cons: ["Slower than LightGBM", "High memory consumption", "Less control over tree structure", "Complex internal encoding"],
  },
  {
    slug: "ann-classification",
    name: "Neural Networks (ANN)",
    category: "classification",
    description: "Artificial Neural Networks are composed of layers of interconnected neurons. Universal function approximators capable of learning any pattern given sufficient data and capacity.",
    complexity: { time: "O(n·d·L·H² · epochs)", space: "O(L·H²)" },
    useCases: ["Image recognition", "Natural language processing", "Speech recognition", "Tabular classification"],
    pros: ["Can learn any function", "Automatic feature learning", "Scalable with data", "State-of-the-art results"],
    cons: ["Requires large data", "Computationally expensive", "Black-box", "Many hyperparameters"],
  },

  // ─── Unsupervised: Clustering ─────────────────────────────────────────────
  {
    slug: "kmeans",
    name: "K-Means",
    category: "clustering",
    description: "Partitions data into K clusters by iteratively assigning points to the nearest centroid and updating centroids as the mean of assigned points.",
    complexity: { time: "O(n·K·I·d)", space: "O(n·K)" },
    useCases: ["Customer segmentation", "Image compression", "Document clustering", "Anomaly detection"],
    pros: ["Simple and fast", "Scales to large datasets", "Guaranteed convergence", "Easy to implement"],
    cons: ["Requires K upfront", "Sensitive to outliers", "Assumes spherical clusters", "Random initialization matters"],
  },
  {
    slug: "kmedoids",
    name: "K-Medoids",
    category: "clustering",
    description: "Similar to K-Means but cluster centers are actual data points (medoids). More robust to outliers and works with arbitrary distance metrics.",
    complexity: { time: "O(n²·K·I)", space: "O(n·K)" },
    useCases: ["Genomic clustering", "Robust customer grouping", "Network analysis", "Medical data clustering"],
    pros: ["Robust to noise and outliers", "Works with non-Euclidean distances", "Interpretable cluster centers", "More stable than K-Means"],
    cons: ["Slower than K-Means", "Computationally expensive", "Requires K upfront", "Sensitive to initialization"],
  },
  {
    slug: "hierarchical-clustering",
    name: "Hierarchical Clustering",
    category: "clustering",
    description: "Builds a tree (dendrogram) of clusters either bottom-up (agglomerative) or top-down (divisive). No need to specify K in advance.",
    complexity: { time: "O(n³) or O(n² log n)", space: "O(n²)" },
    useCases: ["Gene expression analysis", "Social network analysis", "Document organization", "Evolutionary biology"],
    pros: ["No need to specify K", "Produces dendrogram", "Works with any distance metric", "Deterministic"],
    cons: ["Very slow on large datasets", "Sensitive to noise", "Cannot undo merges (agglomerative)", "Memory intensive"],
  },
  {
    slug: "dbscan",
    name: "DBSCAN",
    category: "clustering",
    description: "Density-Based Spatial Clustering discovers clusters of arbitrary shape by grouping together points that are closely packed. Points in low-density regions are classified as noise.",
    complexity: { time: "O(n log n)", space: "O(n)" },
    useCases: ["Geospatial analysis", "Anomaly detection", "Arbitrary-shaped clusters", "Traffic pattern analysis"],
    pros: ["Discovers arbitrary shapes", "Identifies outliers", "No K required", "Robust to noise"],
    cons: ["Struggles with varying density", "Sensitive to epsilon and minSamples", "Not great for high dimensions", "Slow on very large data"],
  },
  {
    slug: "mean-shift",
    name: "Mean Shift",
    category: "clustering",
    description: "A non-parametric clustering algorithm that iteratively shifts each data point toward the mode (peak) of the local density estimate. Automatically finds the number of clusters.",
    complexity: { time: "O(n²·I)", space: "O(n)" },
    useCases: ["Image segmentation", "Object tracking", "Color quantization", "Scientific data analysis"],
    pros: ["Automatically determines K", "Model-free (no assumptions)", "Robust to outliers", "Finds blob-shaped clusters"],
    cons: ["Computationally expensive", "Bandwidth selection is critical", "Not scalable to large datasets", "Slow convergence"],
  },
  {
    slug: "gmm",
    name: "Gaussian Mixture Model (GMM)",
    category: "clustering",
    description: "Models data as a mixture of K Gaussian distributions. Uses the Expectation-Maximization (EM) algorithm to learn parameters. Provides soft/probabilistic cluster assignments.",
    complexity: { time: "O(n·K·d²·I)", space: "O(K·d²)" },
    useCases: ["Probabilistic clustering", "Density estimation", "Speech modeling", "Anomaly detection"],
    pros: ["Soft cluster assignments", "Captures elliptical clusters", "Probabilistic framework", "More flexible than K-Means"],
    cons: ["Requires K upfront", "Sensitive to initialization", "Can get stuck in local optima", "Assumes Gaussian distribution"],
  },

  // ─── Dimensionality Reduction ─────────────────────────────────────────────
  {
    slug: "pca",
    name: "Principal Component Analysis (PCA)",
    category: "dimensionality-reduction",
    description: "Finds orthogonal axes (principal components) of maximum variance in data. Projects data onto a lower-dimensional space while preserving as much variance as possible.",
    complexity: { time: "O(n·d² + d³)", space: "O(d²)" },
    useCases: ["Data visualization", "Noise reduction", "Feature extraction", "Image compression"],
    pros: ["Fast and well-understood", "Removes correlated features", "Reduces overfitting", "Interpretable components"],
    cons: ["Linear only", "Loses some information", "Sensitive to scaling", "Components not always interpretable"],
  },
  {
    slug: "lda",
    name: "Linear Discriminant Analysis (LDA)",
    category: "dimensionality-reduction",
    description: "A supervised dimensionality reduction technique that finds linear combinations of features that best separate classes. Maximizes between-class scatter while minimizing within-class scatter.",
    complexity: { time: "O(n·d²)", space: "O(d²)" },
    useCases: ["Face recognition", "Medical diagnosis", "Pattern recognition", "Marketing segmentation"],
    pros: ["Uses class label information", "Maximizes class separability", "Reduces dimensionality and classifies", "Fast computation"],
    cons: ["Assumes Gaussian class distributions", "Sensitive to outliers", "Requires class labels", "Limited to C-1 dimensions"],
  },
  {
    slug: "tsne",
    name: "t-SNE",
    category: "dimensionality-reduction",
    description: "t-Distributed Stochastic Neighbor Embedding maps high-dimensional data to 2D or 3D for visualization. Preserves local neighborhood structure by modeling pairwise similarities.",
    complexity: { time: "O(n²)", space: "O(n²)" },
    useCases: ["High-dimensional data visualization", "Single-cell RNA analysis", "Word embedding visualization", "Cluster exploration"],
    pros: ["Excellent visualization", "Preserves local structure", "Reveals clusters", "Non-linear"],
    cons: ["Cannot generalize to new data", "Very slow on large datasets", "Non-deterministic", "Hyperparameter sensitive (perplexity)"],
  },
  {
    slug: "umap",
    name: "UMAP",
    category: "dimensionality-reduction",
    description: "Uniform Manifold Approximation and Projection uses Riemannian geometry and algebraic topology to project data. Preserves both local and global structure better than t-SNE.",
    complexity: { time: "O(n^1.14) approx", space: "O(n)" },
    useCases: ["Bioinformatics visualization", "NLP embeddings", "Feature extraction pipeline", "Anomaly detection"],
    pros: ["Much faster than t-SNE", "Preserves global structure", "Can generalize to new data", "Fewer hyperparameters"],
    cons: ["Less interpretable than PCA", "Non-deterministic", "Hyperparameter dependent", "Relatively new (less research)"],
  },
  {
    slug: "autoencoder",
    name: "Autoencoders",
    category: "dimensionality-reduction",
    description: "A neural network that learns to compress data into a latent representation (encoder) and then reconstruct it (decoder). The bottleneck layer acts as the reduced representation.",
    complexity: { time: "O(n·L·H²·epochs)", space: "O(L·H²)" },
    useCases: ["Image compression", "Anomaly detection", "Generative modeling", "Denoising"],
    pros: ["Can learn non-linear representations", "Flexible architecture", "Unsupervised", "Generates new data"],
    cons: ["Requires large data", "Computationally expensive", "Hard to interpret latent space", "Hyperparameter sensitive"],
  },

  // ─── Association Rule Learning ────────────────────────────────────────────
  {
    slug: "apriori",
    name: "Apriori",
    category: "association-rule",
    description: "Discovers frequent itemsets and association rules in transaction data using a breadth-first search. Uses the 'apriori principle': subsets of frequent itemsets must also be frequent.",
    complexity: { time: "O(2^d)", space: "O(2^d)" },
    useCases: ["Market basket analysis", "Medical co-occurrence", "Web usage mining", "Recommendation systems"],
    pros: ["Easy to understand and implement", "Interpretable rules", "Prunes search space efficiently", "Well-studied"],
    cons: ["Exponential in number of items", "Slow on large datasets", "Requires multiple DB scans", "Many spurious rules"],
  },
  {
    slug: "eclat",
    name: "Eclat",
    category: "association-rule",
    description: "Equivalence Class Clustering and bottom-up Lattice Traversal uses vertical data format (tidsets) and depth-first search to discover frequent itemsets faster than Apriori.",
    complexity: { time: "O(2^d)", space: "O(n·d)" },
    useCases: ["Supermarket transaction mining", "Product recommendations", "Network intrusion detection", "Bioinformatics"],
    pros: ["Faster than Apriori on dense data", "Fewer database scans", "Memory efficient with tidsets", "Depth-first search"],
    cons: ["Memory intensive with large tidsets", "Converts to horizontal for rules", "Less popular than Apriori/FP-Growth", "Complex implementation"],
  },
  {
    slug: "fp-growth",
    name: "FP-Growth",
    category: "association-rule",
    description: "Mines frequent patterns without candidate generation by compressing the database into a Frequent Pattern Tree (FP-Tree) and recursively extracting patterns.",
    complexity: { time: "O(n·d)", space: "O(n·d)" },
    useCases: ["Large-scale market basket analysis", "Web log analysis", "Intrusion detection", "Mobile data mining"],
    pros: ["Much faster than Apriori", "No candidate generation", "Only 2 database scans", "Compressed representation"],
    cons: ["FP-Tree can be large", "Complex to implement", "Difficult to update incrementally", "Memory intensive for sparse data"],
  },

  // ─── Semi-Supervised Learning ─────────────────────────────────────────────
  {
    slug: "self-training",
    name: "Self-Training",
    category: "semi-supervised",
    description: "A wrapper algorithm where a classifier is first trained on labeled data, then iteratively adds the most confidently predicted unlabeled examples to the training set.",
    complexity: { time: "O(n·T·base_complexity)", space: "O(n)" },
    useCases: ["Text classification with few labels", "Medical image labeling", "Satellite image classification", "Sentiment analysis"],
    pros: ["Simple to implement", "Works with any base classifier", "Reduces labeling cost", "Iteratively improves"],
    cons: ["Error reinforcement (confirmation bias)", "Sensitive to initial model quality", "Assumes high confidence = correct", "Slow with large unlabeled sets"],
  },
  {
    slug: "label-propagation",
    name: "Label Propagation",
    category: "semi-supervised",
    description: "A graph-based algorithm that propagates labels from labeled nodes to unlabeled nodes through a similarity graph, allowing labels to spread based on data structure.",
    complexity: { time: "O(n²·I)", space: "O(n²)" },
    useCases: ["Social network analysis", "Image segmentation", "Drug-target interaction", "Citation network classification"],
    pros: ["Leverages data manifold structure", "No model assumptions", "Works well on graphs", "Smooth predictions"],
    cons: ["Requires constructing a graph", "Slow on large datasets", "Sensitive to graph construction", "All labels can become one class"],
  },
  {
    slug: "label-spreading",
    name: "Label Spreading",
    category: "semi-supervised",
    description: "A variant of Label Propagation that incorporates regularization by allowing labels to change during propagation. More robust to noisy labels.",
    complexity: { time: "O(n²·I)", space: "O(n²)" },
    useCases: ["Noisy label scenarios", "Text categorization", "Image labeling", "Semi-supervised clustering"],
    pros: ["More robust than Label Propagation", "Handles label noise", "Transductive learning", "Flexible clamping factor"],
    cons: ["Memory intensive (O(n²) matrix)", "Cannot classify unseen data directly", "Graph construction is critical", "Slow for very large datasets"],
  },

  // ─── Reinforcement Learning ───────────────────────────────────────────────
  {
    slug: "q-learning",
    name: "Q-Learning",
    category: "reinforcement-learning",
    description: "A model-free off-policy reinforcement learning algorithm that learns the value of action-state pairs (Q-values) to find the optimal policy without a model of the environment.",
    complexity: { time: "O(|S|·|A|·episodes)", space: "O(|S|·|A|)" },
    useCases: ["Game playing (Atari)", "Robot navigation", "Traffic signal control", "Trading strategies"],
    pros: ["Model-free (no environment model needed)", "Off-policy (learns from any data)", "Guaranteed convergence", "Simple table representation"],
    cons: ["Tabular: doesn't scale to large state spaces", "Slow convergence", "Requires discretization", "Sensitive to learning rate"],
  },
  {
    slug: "sarsa",
    name: "SARSA",
    category: "reinforcement-learning",
    description: "State-Action-Reward-State-Action is an on-policy temporal difference learning algorithm. It updates Q-values based on the action actually taken, making it more conservative than Q-Learning.",
    complexity: { time: "O(|S|·|A|·episodes)", space: "O(|S|·|A|)" },
    useCases: ["Safe robot control", "On-policy game playing", "Sequential decision making", "Risk-sensitive tasks"],
    pros: ["On-policy (safer exploration)", "Converges under proper conditions", "Good for risky environments", "Simple implementation"],
    cons: ["Suboptimal policy in stochastic environments", "Slower convergence than Q-Learning", "Tabular limitations", "Exploration strategy matters greatly"],
  },
  {
    slug: "dqn",
    name: "Deep Q-Network (DQN)",
    category: "reinforcement-learning",
    description: "Combines Q-Learning with deep neural networks to handle high-dimensional state spaces. Uses experience replay and a target network to stabilize training.",
    complexity: { time: "O(n·L·H²·episodes)", space: "O(buffer·state_dim)" },
    useCases: ["Atari game playing", "Robotics", "Autonomous driving", "Resource management"],
    pros: ["Handles image/complex state inputs", "Human-level game performance", "Experience replay for efficiency", "State-of-the-art for discrete actions"],
    cons: ["Discrete actions only", "Requires large replay buffer", "Unstable training", "Many hyperparameters"],
  },
  {
    slug: "policy-gradient",
    name: "Policy Gradient",
    category: "reinforcement-learning",
    description: "Directly optimizes the policy parameters using gradient ascent on the expected cumulative reward, without needing a value function.",
    complexity: { time: "O(episodes·trajectory_length·network)", space: "O(policy_params)" },
    useCases: ["Continuous action spaces", "Robotics control", "NLP with RL", "Game playing"],
    pros: ["Handles continuous action spaces", "Can parametrize stochastic policies", "Direct policy optimization", "End-to-end differentiable"],
    cons: ["High variance gradients", "Sample inefficient", "Local optima", "Sensitive to learning rate"],
  },
  {
    slug: "actor-critic",
    name: "Actor-Critic",
    category: "reinforcement-learning",
    description: "Combines policy gradient (actor) with a value function (critic). The critic evaluates actions taken by the actor, reducing variance while maintaining a policy gradient approach.",
    complexity: { time: "O(episodes·L·H²)", space: "O(actor_params + critic_params)" },
    useCases: ["Continuous control tasks", "Robotics", "OpenAI Gym benchmarks", "Simulated driving"],
    pros: ["Lower variance than pure policy gradient", "Faster convergence", "Works with continuous actions", "Flexible architecture"],
    cons: ["More complex to implement", "Sensitive to hyperparameters", "Critic bias can affect actor", "Can be unstable"],
  },
  {
    slug: "ppo",
    name: "Proximal Policy Optimization (PPO)",
    category: "reinforcement-learning",
    description: "A state-of-the-art policy gradient algorithm that uses a clipped surrogate objective to prevent large, destabilizing policy updates. Balances performance with simplicity.",
    complexity: { time: "O(episodes·L·H²)", space: "O(policy_params)" },
    useCases: ["OpenAI Dota 2", "Robotics control", "Continuous action RL benchmarks", "Game playing"],
    pros: ["Stable and reliable", "Sample efficient", "Easy to tune", "Works on discrete and continuous actions"],
    cons: ["Still sample inefficient vs. model-based RL", "Clip ratio needs tuning", "Multiple epochs of updates needed", "Complex implementation"],
  },

  // ─── Deep Learning ────────────────────────────────────────────────────────
  {
    slug: "ann",
    name: "Artificial Neural Networks (ANN)",
    category: "deep-learning",
    description: "The foundation of deep learning: layers of artificial neurons connected by learnable weights. Trained via backpropagation and gradient descent.",
    complexity: { time: "O(n·d·L·H²·epochs)", space: "O(L·H²)" },
    useCases: ["Tabular data classification", "Regression tasks", "Function approximation", "Feature learning"],
    pros: ["Universal function approximators", "Learns representations automatically", "Highly scalable", "Flexible architecture"],
    cons: ["Requires lots of data", "Black-box", "Computationally expensive", "Prone to overfitting without regularization"],
  },
  {
    slug: "cnn",
    name: "Convolutional Neural Networks (CNN)",
    category: "deep-learning",
    description: "Use convolutional layers with learned filters to exploit spatial hierarchy in data. Parameter sharing and local connectivity make them ideal for image data.",
    complexity: { time: "O(n·K·C·H·W·epochs)", space: "O(K·C·d·d)" },
    useCases: ["Image classification", "Object detection", "Medical imaging", "Video analysis"],
    pros: ["Translation invariant", "Parameter sharing reduces parameters", "State-of-the-art on images", "Hierarchical feature learning"],
    cons: ["Requires large labeled datasets", "Computationally expensive", "Not great for sequences", "Interpretability challenges"],
  },
  {
    slug: "rnn",
    name: "Recurrent Neural Networks (RNN)",
    category: "deep-learning",
    description: "Process sequential data by maintaining a hidden state that summarizes past information, fed back as input at each time step.",
    complexity: { time: "O(n·T·H²·epochs)", space: "O(H²)" },
    useCases: ["Language modeling", "Time series forecasting", "Machine translation", "Speech recognition"],
    pros: ["Handles variable-length sequences", "Shares parameters across time steps", "Captures temporal dependencies", "naturally sequential"],
    cons: ["Vanishing gradient problem", "Slow training (sequential)", "Limited memory (context window)", "Hard to parallelize"],
  },
  {
    slug: "lstm",
    name: "LSTM",
    category: "deep-learning",
    description: "Long Short-Term Memory networks solve the vanishing gradient problem using forget, input, and output gates that selectively remember or forget information over long sequences.",
    complexity: { time: "O(n·T·H²·epochs)", space: "O(H²)" },
    useCases: ["Speech recognition", "Text generation", "Machine translation", "Time series anomaly detection"],
    pros: ["Captures long-term dependencies", "Resistant to vanishing gradients", "Industry standard for sequences", "Interpretable gates"],
    cons: ["Computationally expensive", "Many parameters", "Still limited context compared to Transformers", "Slow training"],
  },
  {
    slug: "gru",
    name: "GRU",
    category: "deep-learning",
    description: "Gated Recurrent Units are a simpler alternative to LSTM with fewer parameters. Uses reset and update gates to control information flow, achieving similar performance with less computation.",
    complexity: { time: "O(n·T·H²·epochs)", space: "O(H²)" },
    useCases: ["Sequence classification", "Natural language processing", "Time series", "Music generation"],
    pros: ["Simpler than LSTM (fewer params)", "Faster training", "Often matches LSTM performance", "Less prone to overfitting"],
    cons: ["May underperform LSTM on very long sequences", "Still sequential", "Limited by fixed hidden size", "Harder to interpret than LSTM"],
  },
  {
    slug: "transformer",
    name: "Transformer",
    category: "deep-learning",
    description: "Relies entirely on self-attention mechanisms to process sequences in parallel. The foundation of modern NLP models (BERT, GPT). Scales exceedingly well with data and compute.",
    complexity: { time: "O(n·T²·d·epochs)", space: "O(T²)" },
    useCases: ["Large language models", "Machine translation", "Text summarization", "Vision Transformers (ViT)"],
    pros: ["Processes all tokens in parallel", "Captures long-range dependencies", "State-of-the-art performance", "Highly scalable"],
    cons: ["Quadratic complexity in sequence length", "Requires massive data", "Very computationally expensive", "Memory intensive"],
  },
  {
    slug: "gan",
    name: "GAN (Generative Adversarial Network)",
    category: "deep-learning",
    description: "A framework where a Generator and Discriminator compete in a minimax game: the Generator creates realistic samples while the Discriminator learns to distinguish real from fake.",
    complexity: { time: "O(n·L·H²·epochs)", space: "O(L·H²)" },
    useCases: ["Image synthesis", "Data augmentation", "Style transfer", "Drug discovery"],
    pros: ["Generates highly realistic samples", "No need for explicit density estimation", "Creative applications", "Flexible architecture"],
    cons: ["Training instability (mode collapse)", "Hard to evaluate quality", "Sensitive to hyperparameters", "Requires large compute"],
  },
];
